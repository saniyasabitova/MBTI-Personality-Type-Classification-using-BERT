{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51O8CrziKuc8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/mbti_2 (1).csv\")\n"
      ],
      "metadata": {
        "id": "kiFln_s0L1yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "3SzkhDPG3vRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['posts'][0]"
      ],
      "metadata": {
        "id": "G2bFvOPYhQXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['posts'][1]\n"
      ],
      "metadata": {
        "id": "pQeqcEXliAfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['type'][1]"
      ],
      "metadata": {
        "id": "v9k-9eb-iHo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "_BlQJYBQM-u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import sys"
      ],
      "metadata": {
        "id": "j7rRad4bNSc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 256\n",
        "train_batch_size = 32\n",
        "valid_batch_size = 32\n",
        "epochs = 2\n",
        "learning_rate = 5e-05\n"
      ],
      "metadata": {
        "id": "rOfUAgI9N-X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "qbly26nLNDM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "waOqmYpEOgwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,df,tokenizer,max_len):\n",
        "    self.df = df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.title = self.df['posts']\n",
        "    self.targets = df[target_columns].values\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.title)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    title = str(self.title[index])\n",
        "    title = ' '.join(title.split())\n",
        "\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "          title,\n",
        "          None,\n",
        "          add_special_tokens = True,\n",
        "          max_length = self.max_len,\n",
        "          padding = 'max_length',\n",
        "          return_token_type_ids = True,\n",
        "          truncation = True,\n",
        "          return_attention_mask = True,\n",
        "          return_tensors = 'pt'\n",
        "          )\n",
        "    target = self.targets[index]\n",
        "\n",
        "        # Если пришло 16 чисел, берем индекс максимального (где стоит 1)\n",
        "    if len(target.shape) > 0 and target.size > 1: # проверяет если это список как [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] и его длина большем чем 1 то np.argmax\n",
        "                                                  # возвращает позицию самого большого, в нашем случае единицы\n",
        "      target = np.argmax(target)\n",
        "    return {\n",
        "          'input_ids': inputs['input_ids'].flatten(),\n",
        "          'attention_mask':  inputs['attention_mask'].flatten(),\n",
        "          'token_type_ids':  inputs['token_type_ids'].flatten(),\n",
        "          'targets': torch.tensor(target, dtype=torch.long)\n",
        "      }"
      ],
      "metadata": {
        "id": "C6s8jDmZPDNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[target_columns].values"
      ],
      "metadata": {
        "id": "ZSQlRbanQLRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded = encoder.fit_transform(df[['type']])\n",
        "\n",
        "\n",
        "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['type']))\n",
        "\n",
        "\n",
        "df = pd.concat([df, encoded_df], axis=1)\n"
      ],
      "metadata": {
        "id": "jsQYnGH6RRFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['type'].unique()"
      ],
      "metadata": {
        "id": "qoFo0WGESSCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_columns = ['type_INFJ', 'type_ENTP', 'type_INTP', 'type_INTJ', 'type_ENTJ', 'type_ENFJ', 'type_INFP', 'type_ENFP', 'type_ISFP', 'type_ISTP', 'type_ISFJ', 'type_ISTJ', 'type_ESTP', 'type_ESFP', 'type_ESTJ', 'type_ESFJ']\n",
        "df"
      ],
      "metadata": {
        "id": "lzGiQJmjR8B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=200)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = val_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "p2OHxtANYTJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_set = Custom_dataset(train_df,tokenizer,max_len)\n",
        "valid_data_set = Custom_dataset(val_df,tokenizer,max_len)"
      ],
      "metadata": {
        "id": "aUVlLqTySlih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch = next(iter(train_data_set))\n",
        "\n",
        "\n",
        "for key, value in batch.items():\n",
        "    print(f\"{key}: {value.shape}\")"
      ],
      "metadata": {
        "id": "u9fXKvglZK1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader  =  torch.utils.data.DataLoader(\n",
        "    train_data_set,\n",
        "    shuffle = True,\n",
        "    batch_size = train_batch_size,\n",
        "    num_workers = 0\n",
        ")\n",
        "valid_data_loader  =  torch.utils.data.DataLoader(\n",
        "    train_data_set,\n",
        "    shuffle = False,\n",
        "    batch_size = valid_batch_size,\n",
        "    num_workers = 0\n",
        ")"
      ],
      "metadata": {
        "id": "jyuPZR-NpxwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ljYlXeX7twwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "  checkpoint = torch. load (checkpoint_fpath)\n",
        "  model.load_state_dict(checkpoint['state _dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer '])\n",
        "  valid_loss_min = checkpoint ['valid_ loss_min']\n",
        "  return model, optimizer, checkpoint ['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    f_path = checkpoint_path\n",
        "    torch.save(state, checkpoint_path)\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "metadata": {
        "id": "57okLqbyuN4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bert_Class(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Bert_Class, self).__init__()\n",
        "    self.bert_model = BertModel.from_pretrained('bert-base-uncased',return_dict = True)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.linear = nn.Linear(768,16)\n",
        "  def forward(self,input_ids,attention_mask,token_type_ids):  #берем значит пуллеры размерностью 768 которые указывают характеристику каждого примера, через линейный слой\n",
        "    output = self.bert_model(input_ids,attention_mask,token_type_ids) #выдаем логиты голосуя к какому классу относится пример, вычисляем лосс функция и повторяем\n",
        "    output_dropout = self.dropout(output.pooler_output)\n",
        "    output =  self.linear(output_dropout)\n",
        "    return output\n",
        "model = Bert_Class()\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "1GANhfzN_H93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def loss_fn(outputs, targets):\n",
        "    return nn.CrossEntropyLoss()(outputs, targets)\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr = learning_rate, eps=1e-8,weight_decay=0.01)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "total_steps = len(train_data_loader) * 5\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "metadata": {
        "id": "S8c3JsGgFVC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, optimizer, checkpoint_path, best_model_path):\n",
        "    valid_loss_min = np.inf\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_loss = 0\n",
        "        valid_loss = 0\n",
        "\n",
        "        print(f'\\\\n--- Эпоха {epoch} из {n_epochs} ---')\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        for index, batch in enumerate(training_loader):\n",
        "            input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "\n",
        "            if index % 10 == 0:\n",
        "                print(f'Батч {index}: Текущий Loss = {loss.item():.4f}')\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for index, batch in enumerate(validation_loader):\n",
        "                input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "                attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "                token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "                targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask, token_type_ids)\n",
        "                loss = loss_fn(outputs, targets)\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "\n",
        "        train_loss = train_loss / len(training_loader)\n",
        "        valid_loss = valid_loss / len(validation_loader)\n",
        "\n",
        "        print(f'ИТОГ ЭПОХИ {epoch}: Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n",
        "\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "        is_best = valid_loss <= valid_loss_min\n",
        "        if is_best:\n",
        "            print(f'!!! Найдена лучшая модель: Valid Loss снизился ({valid_loss_min:.4f} --> {valid_loss:.4f}). Сохраняю...')\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "        save_ckp(checkpoint, is_best, checkpoint_path, best_model_path)\n",
        "\n",
        "\n",
        "    print(f'\\\\nОбучение завершено. Загружаю лучшие веса из {best_model_path}')\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "WWdgyH_hG5hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EPOCHS = 10 # Начни с 3, чтобы проверить, как идет процесс\n",
        "CHECKPOINT_PATH = \"current_checkpoint.pt\"\n",
        "BEST_MODEL_PATH = \"best_model.pt\"\n",
        "\n",
        "trained_model = train_model(\n",
        "    n_epochs=EPOCHS,\n",
        "    training_loader=train_data_loader ,\n",
        "    validation_loader=valid_data_loader, # И этот тоже\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    best_model_path=BEST_MODEL_PATH\n",
        ")\n",
        "\n",
        "print(\"Обучение завершено!\")"
      ],
      "metadata": {
        "id": "Oq3bN8nwwWOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vUHeVqOae9GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wx6XKcnhNhmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(data_loader, model, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"МЕТРИКИ НА ВАЛИДАЦИОННОЙ ВЫБОРКЕ\")\n",
        "    print(\"=\"*50)\n",
        "    print(classification_report(all_targets, all_preds))\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(confusion_matrix(all_targets, all_preds), annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Матрица ошибок (Confusion Matrix)')\n",
        "    plt.xlabel('Предсказанный класс')\n",
        "    plt.ylabel('Истинный класс')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "evaluate_model(valid_data_loader, model, device)"
      ],
      "metadata": {
        "id": "iYVXZVH2fAM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}